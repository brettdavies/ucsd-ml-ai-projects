{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ed9cec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('../hf.env')\n",
        "HF_TOKEN_PATH = os.getenv(\"HF_TOKEN_PATH\")\n",
        "HF_MODEL_CACHE = os.getenv(\"HF_MODEL_CACHE\")\n",
        "HF_DATASETS_CACHE = os.getenv(\"HF_DATASETS_CACHE\")\n",
        "\n",
        "print(f\"{HF_DATASETS_CACHE}\")\n",
        "\n",
        "# Read the token if it is available\n",
        "try:\n",
        "    with open(HF_TOKEN_PATH, 'r') as token_file:\n",
        "        HF_TOKEN = token_file.read().strip()\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [],
      "source": [
        "# Load and prepare the Common Voice dataset for Hindi\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", token=\"use_auth_token\", trust_remote_code=True, keep_in_memory=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", token=\"use_auth_token\", trust_remote_code=True, keep_in_memory=True)\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
      "metadata": {
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce"
      },
      "outputs": [],
      "source": [
        "# Remove columns that are not required\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [],
      "source": [
        "# Initialize the Whisper feature extractor\n",
        "# The feature extractor preprocesses audio inputs\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90d056e20b3e4f14ae0199a1a4ab1bb0",
            "d82a88daec0e4f14add691b7b903064c",
            "350acdb0f40e454099fa901e66de55f0",
            "2e6a82a462cc411d90fa1bea4ee60790",
            "c74bfee0198b4817832ea86e8e88d96c",
            "04fb2d81eff646068e10475a08ae42f4"
          ]
        },
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
      },
      "outputs": [],
      "source": [
        "# Initialize the Whisper tokenizer for Hindi\n",
        "# The tokenizer converts text to token ids and vice versa\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [],
      "source": [
        "# Combine feature extractor and tokenizer into a single processor\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\", tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
      "metadata": {
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"
      },
      "outputs": [],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87122d71-289a-466a-afcf-fa354b18946b",
      "metadata": {
        "id": "87122d71-289a-466a-afcf-fa354b18946b"
      },
      "outputs": [],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "# Define a function to prepare the dataset\n",
        "# This function processes audio data and encodes transcriptions\n",
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2,\n",
        "                                 cache_file_names={\n",
        "                                    \"train\": os.path.join(HF_DATASETS_CACHE, \"mozilla-foundation___common_voice_11_0\",\"hi\",\"train.arrow\"),\n",
        "                                    \"test\": os.path.join(HF_DATASETS_CACHE, \"mozilla-foundation___common_voice_11_0\",\"hi\",\"test.arrow\")\n",
        "                                 }\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
      "metadata": {
        "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
      },
      "outputs": [],
      "source": [
        "model.generation_config.language = \"hindi\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperTokenizer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "\n",
        "# Define the compute metrics function\n",
        "# metric = evaluate.load(\"wer\")\n",
        "metric = evaluate.load(\"wer\", trust_remote_code=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions\n",
        "    \n",
        "    # Replace -100 with the pad_token_id in labels\n",
        "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode the predictions and labels to texts\n",
        "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Compute WER\n",
        "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
        "    \n",
        "    # Return the metrics\n",
        "    return {\n",
        "        \"wer\": wer,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e379f4cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class AccuracyLoggerCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_accuracies = []\n",
        "        self.eval_accuracies = []\n",
        "        self.epochs = []\n",
        "        print(\"AccuracyLoggerCallback initialized\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        print(\"on_log called\")\n",
        "        print(\"on_log Logs type:\", type(logs))  # Debugging print to check log history content\n",
        "        print(\"on_log Logs:\", logs)  # Debugging print to check log history content\n",
        "        print(\"on_log Log History:\", state.log_history)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_log state type:\", type(state))  # Debugging print to check log history content\n",
        "        print(\"on_log state:\", state)  # Debugging print to check log history content\n",
        "        print(\"on_log state epocs:\", state.epoch)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_log self type:\", type(self))  # Debugging print to check log history content\n",
        "        print(\"on_log self:\", self)  # Debugging print to check log history content\n",
        "        print(\"on_log self epocs:\", self.epochs)  # Debugging print to check log history content\n",
        "\n",
        "        if logs is not None and \"eval_wer\" in logs:\n",
        "            self.epochs.append(state.epoch)\n",
        "            self.eval_accuracies.append(1 - logs[\"eval_wer\"] / 100)  # converting WER to accuracy\n",
        "            print(f\"Eval WER: {logs['eval_wer']}, Accuracy: {1 - logs['eval_wer'] / 100}\")\n",
        "            self.plot_accuracies()\n",
        "\n",
        "        if logs is not None and \"train_wer\" in logs:\n",
        "            self.train_accuracies.append(1 - logs[\"train_wer\"] / 100)  # converting WER to accuracy\n",
        "            print(f\"Train WER: {logs['train_wer']}, Accuracy: {1 - logs['train_wer'] / 100}\")\n",
        "            self.plot_accuracies()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        print(\"on_epoch_end called\")\n",
        "        self.epochs.append(state.epoch)\n",
        "        \n",
        "        print(\"on_epoch_end state type:\", type(state))  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end state:\", state)  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end state epocs:\", state.epoch)  # Debugging print to check log history content\n",
        "\n",
        "        print(\"on_epoch_end self type:\", type(self))  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end self:\", self)  # Debugging print to check log history content\n",
        "        print(\"on_epoch_end self epocs:\", self.epochs)  # Debugging print to check log history content\n",
        "\n",
        "        # Get training accuracy (for this example, we use WER and convert to accuracy)\n",
        "        if state.log_history:\n",
        "            train_logs = state.log_history[-1]\n",
        "            if \"train_wer\" in train_logs:\n",
        "                train_wer = train_logs[\"train_wer\"]\n",
        "                self.train_accuracies.append(1 - train_wer / 100)  # converting WER to accuracy\n",
        "                print(f\"Train WER Epoch: {state.epoch}, Accuracy: {(1 - train_wer / 100)}\")\n",
        "\n",
        "            # Get evaluation accuracy\n",
        "            if \"eval_wer\" in train_logs:\n",
        "                eval_wer = train_logs[\"eval_wer\"]\n",
        "                self.eval_accuracies.append(1 - eval_wer / 100)  # converting WER to accuracy\n",
        "                print(f\"Eval WER Epoch: {state.epoch}, Accuracy: {(1 - eval_wer / 100)}\")\n",
        "\n",
        "            # Plot the accuracies after each epoch\n",
        "            self.plot_accuracies()\n",
        "        else:\n",
        "            print(\"No log history found at epoch end\")\n",
        "\n",
        "    def plot_accuracies(self):\n",
        "        # clear_output(wait=True)\n",
        "        # Debugging print statements\n",
        "        print(f\"Plotting - Train Epochs: {self.epochs}, Accuracies: {self.train_accuracies}\")\n",
        "        print(f\"Plotting - Eval Epochs: {self.epochs}, Accuracies: {self.eval_accuracies}\")\n",
        "        \n",
        "        train_length_match = len(self.epochs) == len(self.train_accuracies)\n",
        "        eval_length_match = len(self.epochs) == len(self.eval_accuracies)\n",
        "\n",
        "        if train_length_match and eval_length_match:\n",
        "            plt.plot(self.epochs, self.train_accuracies, label='Training Accuracy', marker='o')\n",
        "            plt.plot(self.epochs, self.eval_accuracies, label='Validation Accuracy', marker='x')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('Training Accuracy over Epochs')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "        else:\n",
        "            if not train_length_match:\n",
        "                print(f\"Mismatch in lengths - Train Epochs: {len(self.epochs)}, Accuracies: {len(self.train_accuracies)}\")\n",
        "            if not eval_length_match:\n",
        "                print(f\"Mismatch in lengths - Eval Epochs: {len(self.epochs)}, Accuracies: {len(self.eval_accuracies)}\")\n",
        "\n",
        "\n",
        "accuracy_logger = AccuracyLoggerCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "# Define training arguments and configuration\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{HF_MODEL_CACHE}/whisper-small-hi\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=1000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_dir=f\"{HF_MODEL_CACHE}/whisper-small-hi/logs\",\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [],
      "source": [
        "# Initialize the Seq2SeqTrainer with model, datasets, and training configuration\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[accuracy_logger]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2zQwMfEOBJq",
      "metadata": {
        "id": "-2zQwMfEOBJq"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [],
      "source": [
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfc3853",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final plot (in case training stops prematurely)\n",
        "accuracy_logger.plot_accuracies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper Small Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bc39b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Clear out relevant environment variables by setting them to null\n",
        "# os.environ['HF_TOKEN'] = ''\n",
        "# os.environ['HUGGING_FACE_HUB_TOKEN'] = ''\n",
        "\n",
        "# # Import and execute the logout function\n",
        "# from huggingface_hub import logout\n",
        "# huggingface_hub.logout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a2114e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear model and processor from memory\n",
        "del model\n",
        "del processor\n",
        "\n",
        "# Manually call the garbage collector\n",
        "gc.collect()\n",
        "\n",
        "# Clear CUDA cache if using GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
