on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.061124694376528114, global_step=25, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.15434160128e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.061124694376528114
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.12224938875305623, global_step=50, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.30868320256e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.12224938875305623
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.18337408312958436, global_step=75, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.46302480384e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.18337408312958436
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.24449877750611246, global_step=100, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61736640512e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.24449877750611246
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.3056234718826406, global_step=125, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=5.7717080064e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.3056234718826406
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.36674816625916873, global_step=150, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=6.92604960768e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.36674816625916873
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.4278728606356968, global_step=175, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=8.08039120896e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.4278728606356968
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.4889975550122249, global_step=200, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=9.23473281024e+17, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.4889975550122249
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.5501222493887531, global_step=225, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.038907441152e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.5501222493887531
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.6112469437652812, global_step=250, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.15434160128e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.6112469437652812
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.6723716381418093, global_step=275, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.269775761408e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.6723716381418093
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.7334963325183375, global_step=300, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.385209921536e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.7334963325183375
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.7946210268948656, global_step=325, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.500644081664e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.7946210268948656
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.8557457212713936, global_step=350, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.616078241792e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.8557457212713936
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.9168704156479217, global_step=375, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.73151240192e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.9168704156479217
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=0.9779951100244498, global_step=400, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.846946562048e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 0.9779951100244498
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: []
on_epoch_end called
on_epoch_end state type: <class 'transformers.trainer_callback.TrainerState'>
on_epoch_end state: TrainerState(epoch=1.0, global_step=409, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.846946562048e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_epoch_end state epocs: 1.0
on_epoch_end self type: <class '__main__.AccuracyLoggerCallback'>
on_epoch_end self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_epoch_end self epocs: [1.0]
Plotting - Train Epochs: [1.0], Accuracies: []
Plotting - Eval Epochs: [1.0], Accuracies: []
Mismatch in lengths - Train Epochs: 1, Accuracies: 0
Mismatch in lengths - Eval Epochs: 1, Accuracies: 0
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.039119804400978, global_step=425, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=1.96122638057472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.039119804400978
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.1002444987775062, global_step=450, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.07666054070272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.1002444987775062
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.1613691931540342, global_step=475, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.19209470083072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.1613691931540342
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.2224938875305624, global_step=500, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.30752886095872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.2224938875305624
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.2836185819070904, global_step=525, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.42296302108672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.2836185819070904
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.3447432762836184, global_step=550, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.53839718121472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.3447432762836184
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.4058679706601467, global_step=575, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.65383134134272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.4058679706601467
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.466992665036675, global_step=600, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.76926550147072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.466992665036675
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.528117359413203, global_step=625, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=2.88469966159872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.528117359413203
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.589242053789731, global_step=650, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.00013382172672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.589242053789731
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.6503667481662592, global_step=675, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.11556798185472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.6503667481662592
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.7114914425427874, global_step=700, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.23100214198272e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.7114914425427874
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.7726161369193154, global_step=725, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.34643630211072e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.7726161369193154
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.8337408312958434, global_step=750, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.46187046223872e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.8337408312958434
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.8948655256723717, global_step=775, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.57730462236672e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.8948655256723717
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=1.9559902200488999, global_step=800, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.69273878249472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 1.9559902200488999
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0]
on_epoch_end called
on_epoch_end state type: <class 'transformers.trainer_callback.TrainerState'>
on_epoch_end state: TrainerState(epoch=2.0, global_step=818, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.69273878249472e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_epoch_end state epocs: 2.0
on_epoch_end self type: <class '__main__.AccuracyLoggerCallback'>
on_epoch_end self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_epoch_end self epocs: [1.0, 2.0]
Plotting - Train Epochs: [1.0, 2.0], Accuracies: []
Plotting - Eval Epochs: [1.0, 2.0], Accuracies: []
Mismatch in lengths - Train Epochs: 2, Accuracies: 0
Mismatch in lengths - Eval Epochs: 2, Accuracies: 0
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.0171149144254277, global_step=825, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.80701860102144e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.0171149144254277
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.078239608801956, global_step=850, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.92245276114944e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.078239608801956
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.139364303178484, global_step=875, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.03788692127744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.139364303178484
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.2004889975550124, global_step=900, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.15332108140544e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.2004889975550124
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.26161369193154, global_step=925, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.26875524153344e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.26161369193154
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.3227383863080684, global_step=950, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.38418940166144e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.3227383863080684
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.3838630806845966, global_step=975, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.49962356178944e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.3838630806845966
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
on_log called
on_log Logs type: <class 'dict'>
on_log Logs: {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125}
on_log Log History: [{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}]
on_log state type: <class 'transformers.trainer_callback.TrainerState'>
on_log state: TrainerState(epoch=2.444987775061125, global_step=1000, max_steps=1000, logging_steps=25, eval_steps=1000, save_steps=1000, train_batch_size=16, num_train_epochs=3, num_input_tokens_seen=0, total_flos=4.61505772191744e+18, log_history=[{'loss': 0.9307, 'grad_norm': 11.914546966552734, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.061124694376528114, 'step': 25}, {'loss': 0.7857, 'grad_norm': 10.61684513092041, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.12224938875305623, 'step': 50}, {'loss': 0.6216, 'grad_norm': 6.783166408538818, 'learning_rate': 1.46e-06, 'epoch': 0.18337408312958436, 'step': 75}, {'loss': 0.5359, 'grad_norm': 6.557876110076904, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.24449877750611246, 'step': 100}, {'loss': 0.4474, 'grad_norm': 5.776425361633301, 'learning_rate': 2.46e-06, 'epoch': 0.3056234718826406, 'step': 125}, {'loss': 0.4173, 'grad_norm': 6.025163173675537, 'learning_rate': 2.96e-06, 'epoch': 0.36674816625916873, 'step': 150}, {'loss': 0.3965, 'grad_norm': 6.389370918273926, 'learning_rate': 3.46e-06, 'epoch': 0.4278728606356968, 'step': 175}, {'loss': 0.3636, 'grad_norm': 6.864609718322754, 'learning_rate': 3.96e-06, 'epoch': 0.4889975550122249, 'step': 200}, {'loss': 0.3324, 'grad_norm': 5.104791641235352, 'learning_rate': 4.4600000000000005e-06, 'epoch': 0.5501222493887531, 'step': 225}, {'loss': 0.3193, 'grad_norm': 5.362447261810303, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.6112469437652812, 'step': 250}, {'loss': 0.3081, 'grad_norm': 6.007716178894043, 'learning_rate': 5.460000000000001e-06, 'epoch': 0.6723716381418093, 'step': 275}, {'loss': 0.2909, 'grad_norm': 5.637144088745117, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.7334963325183375, 'step': 300}, {'loss': 0.2753, 'grad_norm': 5.092892169952393, 'learning_rate': 6.460000000000001e-06, 'epoch': 0.7946210268948656, 'step': 325}, {'loss': 0.2787, 'grad_norm': 5.302077770233154, 'learning_rate': 6.96e-06, 'epoch': 0.8557457212713936, 'step': 350}, {'loss': 0.27, 'grad_norm': 3.941859483718872, 'learning_rate': 7.4600000000000006e-06, 'epoch': 0.9168704156479217, 'step': 375}, {'loss': 0.2595, 'grad_norm': 5.595149040222168, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.9779951100244498, 'step': 400}, {'loss': 0.2222, 'grad_norm': 4.519338607788086, 'learning_rate': 8.46e-06, 'epoch': 1.039119804400978, 'step': 425}, {'loss': 0.1978, 'grad_norm': 3.9345741271972656, 'learning_rate': 8.96e-06, 'epoch': 1.1002444987775062, 'step': 450}, {'loss': 0.2012, 'grad_norm': 4.153043746948242, 'learning_rate': 9.460000000000001e-06, 'epoch': 1.1613691931540342, 'step': 475}, {'loss': 0.1911, 'grad_norm': 5.046008586883545, 'learning_rate': 9.960000000000001e-06, 'epoch': 1.2224938875305624, 'step': 500}, {'loss': 0.1903, 'grad_norm': 4.92111349105835, 'learning_rate': 9.54e-06, 'epoch': 1.2836185819070904, 'step': 525}, {'loss': 0.1881, 'grad_norm': 4.020190238952637, 'learning_rate': 9.040000000000002e-06, 'epoch': 1.3447432762836184, 'step': 550}, {'loss': 0.176, 'grad_norm': 4.5838470458984375, 'learning_rate': 8.540000000000001e-06, 'epoch': 1.4058679706601467, 'step': 575}, {'loss': 0.1774, 'grad_norm': 4.701812744140625, 'learning_rate': 8.040000000000001e-06, 'epoch': 1.466992665036675, 'step': 600}, {'loss': 0.173, 'grad_norm': 3.6823644638061523, 'learning_rate': 7.540000000000001e-06, 'epoch': 1.528117359413203, 'step': 625}, {'loss': 0.168, 'grad_norm': 4.936620235443115, 'learning_rate': 7.04e-06, 'epoch': 1.589242053789731, 'step': 650}, {'loss': 0.174, 'grad_norm': 4.7141618728637695, 'learning_rate': 6.540000000000001e-06, 'epoch': 1.6503667481662592, 'step': 675}, {'loss': 0.1632, 'grad_norm': 4.408812999725342, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.7114914425427874, 'step': 700}, {'loss': 0.1466, 'grad_norm': 3.702017068862915, 'learning_rate': 5.540000000000001e-06, 'epoch': 1.7726161369193154, 'step': 725}, {'loss': 0.1696, 'grad_norm': 5.809318542480469, 'learning_rate': 5.04e-06, 'epoch': 1.8337408312958434, 'step': 750}, {'loss': 0.1547, 'grad_norm': 4.892189025878906, 'learning_rate': 4.540000000000001e-06, 'epoch': 1.8948655256723717, 'step': 775}, {'loss': 0.1554, 'grad_norm': 3.942331075668335, 'learning_rate': 4.04e-06, 'epoch': 1.9559902200488999, 'step': 800}, {'loss': 0.1258, 'grad_norm': 2.8695497512817383, 'learning_rate': 3.54e-06, 'epoch': 2.0171149144254277, 'step': 825}, {'loss': 0.0887, 'grad_norm': 2.9501161575317383, 'learning_rate': 3.04e-06, 'epoch': 2.078239608801956, 'step': 850}, {'loss': 0.0829, 'grad_norm': 3.089911460876465, 'learning_rate': 2.5400000000000002e-06, 'epoch': 2.139364303178484, 'step': 875}, {'loss': 0.086, 'grad_norm': 3.160684823989868, 'learning_rate': 2.04e-06, 'epoch': 2.2004889975550124, 'step': 900}, {'loss': 0.08, 'grad_norm': 2.787513017654419, 'learning_rate': 1.54e-06, 'epoch': 2.26161369193154, 'step': 925}, {'loss': 0.0838, 'grad_norm': 2.7299702167510986, 'learning_rate': 1.04e-06, 'epoch': 2.3227383863080684, 'step': 950}, {'loss': 0.084, 'grad_norm': 2.504974365234375, 'learning_rate': 5.4e-07, 'epoch': 2.3838630806845966, 'step': 975}, {'loss': 0.082, 'grad_norm': 3.0979137420654297, 'learning_rate': 4e-08, 'epoch': 2.444987775061125, 'step': 1000}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})
on_log state epocs: 2.444987775061125
on_log self type: <class '__main__.AccuracyLoggerCallback'>
on_log self: <__main__.AccuracyLoggerCallback object at 0x7217b62d2110>
on_log self epocs: [1.0, 2.0]
